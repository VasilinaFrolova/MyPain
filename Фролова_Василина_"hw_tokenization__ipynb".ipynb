{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasilinaFrolova/MyPain/blob/main/%D0%A4%D1%80%D0%BE%D0%BB%D0%BE%D0%B2%D0%B0_%D0%92%D0%B0%D1%81%D0%B8%D0%BB%D0%B8%D0%BD%D0%B0_%22hw_tokenization__ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\" \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\" \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\" \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "\n",
        "import re\n",
        "def tokenize_by_spaces_and_punctuation(text):\n",
        "  tokens = re.findall(r'\\w+|\\$[\\d,]+\\.?\\d*|[^\\w\\s]', text)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tokens = tokenize_by_spaces_and_punctuation(text)\n",
        "print(\"Токены:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8114f1e1-3745-4bb7-c7a2-8ec7614e2931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токены:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$1,000.50', '.', 'I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.', 'What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\" \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\" \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\" \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f6edb7-cf26-499b-fdf6-3efef5264f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50.I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it.What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "['The quick brown fox jumps over the lazy dog.', \"It's a beautiful day!Dr.\", 'Smith arrived at 5:30 p.m. from New York.', \"The meeting cost $1,000.50.I can't believe she's going!\", \"Let's meet at Jane's house.\", \"They'll love it.What's the ETA for the package?\", 'Please e-mail support@example.com ASAP!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy. load(\"en_core_web_sm\")\n",
        "text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\" \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\" \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\" \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ZdWmbVHeJe",
        "outputId": "15ba2390-a846-401a-e321-7ff702ade619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day!Dr', '.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50.I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d53e3b2-94b4-4062-b94f-90d033df2e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regex токены:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$1,000.50', '.', 'I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.', 'What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n",
            "NLTK токены:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50.I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it.What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "spaCy токены:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day!Dr', '.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50.I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "texts = [\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\" \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\" \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\" \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"]\n",
        "def tokenize_by_spaces_and_punctuation(text):\n",
        "    tokens = re.findall(r'\\w+|\\$[\\d,]+\\.?\\d*|[^\\w\\s]', text)\n",
        "    return tokens\n",
        "\n",
        "for text in texts:\n",
        "    tokens1 = tokenize_by_spaces_and_punctuation(text)\n",
        "    print(\"Regex токены:\")\n",
        "    print(tokens1)\n",
        "\n",
        "    tokens2 = nltk.word_tokenize(text)\n",
        "    print(\"NLTK токены:\")\n",
        "    print(tokens2)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    tokens3 = [token.text for token in doc]\n",
        "    print(\"spaCy токены:\")\n",
        "    print(tokens3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Разделение по пробелам и знакам препинания не может быть достаточным для сложных или многозначных конструкций.\n",
        "Пример №1: Фразовые глаголы и идиомы - \"give up\" (сдаваться), \"look forward to\" (ждать с нетерпением) должны обрабатываться как единые смысловые единицы, а не как отдельные слова. Простая токенизация разбивает их на [\"give\", \"up\"], теряя смысл.\n",
        "Пример №2: Переносные значения - \"hot dog\" (хот-дог) vs \"hot\" (горячий) + \"dog\" (собака). Без контекстного анализа невозможно определить истинное значение.\n",
        "Разделение по пробелам и знакам препинания не может быть достаточным для составных слов или для специфических форм слов.\n",
        "Пример №1: \"can't\" → ['can', \"'\", 't'] при простом разделении, хотя правильно [\"can\", \"not\"]. Аналогично \"gonna\", \"wanna\".\n",
        "Пример №2: \"Fahrkarte\" (проездной билет) должен разбиваться на морфемы, а не на отдельные буквы или оставаться одним длинным словом.\n",
        "2. Во фразе «You shall know a word by the company it keeps» 10 токенов.\n",
        "You — 1 токен,\n",
        "shall — 1 токен,\n",
        "know — 1 токен,\n",
        "a — 1 токен,\n",
        "word — 1 токен,\n",
        "by — 1 токен,\n",
        "the — 1 токен,\n",
        "company — 1 токен,\n",
        "it — 1 токен,\n",
        "keeps — 1 токен.\n",
        "Значение получено при помощи gpt-tokenizer, использована модель gpt-5.\n",
        "3. (BPE) - пошаговое описание:\n",
        "Исходная подготовка:\n",
        "Текст разбивается на отдельные символы или байты.\n",
        "Создается начальный словарь из всех уникальных символов.\n",
        "Подсчитываются частоты всех пар соседних символов.\n",
        "Пример работы:\n",
        "Исходный текст: \"low lower lowest\"\n",
        "Начальные токены: l, o, w,  , l, o, w, e, r,  , l, o, w, e, s, t\n",
        "Шаг 1: \"lo\" → самый частый биграм, создаем токен \"lo\"\n",
        "Токены после шага 1: lo, w, _, lo, w, e, r, _, lo, w, e, s, t\n",
        "Шаг 2: \"low\" → частый триграм, создаем токен \"low\"\n",
        "И т.д.\n",
        "BPE \"учится\" на тексте, постепенно объединяя частые сочетания символов в осмысленные субсловные единицы.\n"
      ],
      "metadata": {
        "id": "R3hxZsb3ZUNR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}